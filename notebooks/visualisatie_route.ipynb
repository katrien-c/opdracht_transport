{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36fea664-7c03-4225-96ab-5b9e5c58bd38",
   "metadata": {},
   "source": [
    "# In de cel na de functies kan je de functie visualize_first_and_second_last() 'callen'. \n",
    "# De 1ste en voorlaatste stand/tijdstip worden weergegeven. De laatste wordt genegeerd omdat deze 'meestal' experimenteel is. \n",
    "# Helaas is dit niet altijd het geval. Hier moet nog aan gewerkt worden om dat te kunnen onderscheiden...\n",
    "# Fout in weergave van nieuwe task: als er onderweg ergens een task wordt gedelete en daarna terug toegevoegd wordt dit aangegeven als nieuwe task \n",
    "Onderaan een klad-versie van een plot van begin tot einde. Een bijverschijnsel is het afdrukken van een ongevraagde GIF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b1431-3258-4c33-b3af-bbbe9fa525b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "#13/01/2026\n",
    "\n",
    "# Static \"Begin vs End\" plot (NO animation, NO gif)\n",
    "# - Plots TWO charts:\n",
    "#     (1) first request of the day (earliest time)\n",
    "#     (2) second-last request of the day (second-latest time)  <-- ignores the \"real\" last one (redundant)\n",
    "#\n",
    "# Shows:\n",
    "#   - all request task coordinates (scatter)\n",
    "#   - response route order (polyline) if matching response TXT exists\n",
    "#   - route length (lon/lat units) in the title block\n",
    "#\n",
    "# Works with the standard naming convention:\n",
    "#   ROUTE-YYYYMMDD-HHMMSS-NTASKS-NFIXED.json / .txt\n",
    "\n",
    "# colors and legend adjustments\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from time import time\n",
    "start1 = time()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Styling knobs (edit these to change colors/markers/line styles)\n",
    "# ------------------------------------------------------------\n",
    "BASE_DOT_COLOR   = \"blue\"   # base task dots\n",
    "NEW_DOT_COLOR    = \"orange\"     # new nodes (END only)\n",
    "DEL_X_COLOR      = \"red\"        # deleted nodes (END only)\n",
    "\n",
    "BASE_ROUTE_COLOR = \"tab:blue\"   # route polyline\n",
    "NEW_EDGE_COLOR   = \"green\"      # new connections overlay\n",
    "\n",
    "BASE_DOT_SIZE    = 12\n",
    "NEW_DOT_SIZE     = 150\n",
    "DEL_X_SIZE       = 80\n",
    "\n",
    "BASE_ROUTE_LW    = 1.0\n",
    "BASE_ROUTE_ALPHA = 0.60\n",
    "NEW_EDGE_LW      = 2.0\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers (simple + readable)\n",
    "# ------------------------------------------------------------\n",
    "# File Name Regular Expression FN_RE\n",
    "FN_RE = re.compile(\n",
    "    r\"^(?P<route>[^-]+)-(?P<date>\\d{8})-(?P<time>\\d{6})-(?P<n_tasks>\\d+)-(?P<n_fixed>\\d+)\\.(?P<ext>json|txt)$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "EARTH_RADIUS_KM = 6371.0088\n",
    "\n",
    "def safe_json_load(p: Path) -> dict:\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "def parse_time_from_filename(name: str) -> str | None:\n",
    "    # 0521_301-20220614-053247-170-0.json -> \"053247\"\n",
    "    parts = name.split(\"-\")\n",
    "    return parts[2] if len(parts) >= 3 else None\n",
    "\n",
    "def node_key(lat: float, lon: float, nd: int = 6) -> str:\n",
    "    return f\"{round(float(lat), nd)}|{round(float(lon), nd)}\"\n",
    "\n",
    "def list_day_files(data_dir: str, subdir: str, route_prefix: str, date_str: str, ext: str) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Supports both layouts:\n",
    "      A) data/requests/*.json\n",
    "      B) data/requests/0521_301-20220614/*.json   <-- your case\n",
    "    \"\"\"\n",
    "    base = Path(data_dir)\n",
    "    root = base / subdir\n",
    "    day_folder = root / f\"{route_prefix}-{date_str}\"\n",
    "\n",
    "    if day_folder.exists():\n",
    "        candidates = list(day_folder.glob(f\"*.{ext}\"))\n",
    "    else:\n",
    "        candidates = list(root.rglob(f\"*.{ext}\"))\n",
    "\n",
    "    prefix = f\"{route_prefix}-{date_str}-\"\n",
    "    files = [p for p in candidates if p.name.startswith(prefix)]\n",
    "    files.sort(key=lambda p: parse_time_from_filename(p.name) or \"999999\")\n",
    "    return files\n",
    "\n",
    "def parse_request_tasks(request_file: Path) -> dict[str, tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Returns: dict task_id -> (lat, lon)\n",
    "    \"\"\"\n",
    "    obj = safe_json_load(request_file)\n",
    "    tasks = {}\n",
    "    for t in obj.get(\"tasks\", []) or []:\n",
    "        tid = str(t.get(\"id\", \"\")).strip()\n",
    "        addr = t.get(\"address\", {}) or {}\n",
    "        lat = float(addr.get(\"latitude\"))\n",
    "        lon = float(addr.get(\"longitude\"))\n",
    "        tasks[tid] = (lat, lon)\n",
    "    return tasks\n",
    "\n",
    "def read_response_tokens(response_file: Path) -> list[str]:\n",
    "    \"\"\"\n",
    "    Response file = list of numbers (task ids) -> list[str]\n",
    "    \"\"\"\n",
    "    return [\n",
    "        ln.strip() for ln in response_file.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "        if ln.strip()\n",
    "    ]\n",
    "\n",
    "def map_response_to_coords(tasks: dict[str, tuple[float, float]], route_ids: list[str]):\n",
    "    \"\"\"\n",
    "    Map response task-id sequence -> coords_latlon list, and count missing mappings.\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    missing = 0\n",
    "    for rid in route_ids:\n",
    "        if rid in tasks:\n",
    "            coords.append(tasks[rid])\n",
    "        else:\n",
    "            missing += 1\n",
    "    return coords, missing\n",
    "\n",
    "def route_length_km(coords_latlon: list[tuple[float, float]]) -> float:\n",
    "    \"\"\"\n",
    "    Great-circle (haversine) distance in kilometers along the route polyline.\n",
    "    \"\"\"\n",
    "    if len(coords_latlon) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    lat_deg = np.array([c[0] for c in coords_latlon], float)\n",
    "    lon_deg = np.array([c[1] for c in coords_latlon], float)\n",
    "\n",
    "    lat = np.radians(lat_deg)\n",
    "    lon = np.radians(lon_deg)\n",
    "\n",
    "    dlat = np.diff(lat)\n",
    "    dlon = np.diff(lon)\n",
    "\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat[:-1]) * np.cos(lat[1:]) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2.0 * np.arctan2(np.sqrt(a), np.sqrt(1.0 - a))\n",
    "\n",
    "    return float(EARTH_RADIUS_KM * np.sum(c))\n",
    "\n",
    "def pretty_title(p: Path) -> str:\n",
    "    # 0521_301-20220614-053247-170-0.json\n",
    "    parts = p.name.split(\"-\")\n",
    "    route = parts[0]\n",
    "    date = parts[1]\n",
    "    t = parts[2]\n",
    "    return f\"{route}  {date[:4]}-{date[4:6]}-{date[6:8]}  {t[:2]}:{t[2:4]}:{t[4:6]}\"\n",
    "\n",
    "def key_map_from_tasks(tasks: dict[str, tuple[float, float]], nd: int = 6) -> dict[str, tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Returns: node_key -> (lat, lon)\n",
    "    If multiple tasks share same coords, key collapses to one (OK for new/deleted node detection).\n",
    "    \"\"\"\n",
    "    km = {}\n",
    "    for (lat, lon) in tasks.values():\n",
    "        km[node_key(lat, lon, nd=nd)] = (lat, lon)\n",
    "    return km\n",
    "\n",
    "def route_segments_from_coords(coords_latlon: list[tuple[float, float]], nd: int = 6) -> set[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Returns set of directed segments (k_i -> k_{i+1}) based on node_key(lat,lon).\n",
    "    \"\"\"\n",
    "    if len(coords_latlon) < 2:\n",
    "        return set()\n",
    "    keys = [node_key(lat, lon, nd=nd) for (lat, lon) in coords_latlon]\n",
    "    segs = set()\n",
    "    for a, b in zip(keys[:-1], keys[1:]):\n",
    "        if a != b:\n",
    "            segs.add((a, b))\n",
    "    return segs\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main: 2 static charts (BEGIN + SECOND_LAST)\n",
    "# ------------------------------------------------------------\n",
    "def visualize_first_and_second_last(data_dir: str, route_prefix: str, date_str: str):\n",
    "    req_files = list_day_files(data_dir, \"requests\", route_prefix, date_str, \"json\")\n",
    "    resp_files = list_day_files(data_dir, \"responses\", route_prefix, date_str, \"txt\")\n",
    "\n",
    "    if len(req_files) < 2:\n",
    "        raise FileNotFoundError(\"Need at least 2 request JSON files for this route/date.\")\n",
    "\n",
    "    resp_by_stem = {p.stem: p for p in resp_files}\n",
    "\n",
    "    first_req = req_files[0]\n",
    "    second_last_req = req_files[-2]   # ignore the \"real\" last one (redundant)\n",
    "\n",
    "    def load_session(req_path: Path):\n",
    "        tasks = parse_request_tasks(req_path)\n",
    "        resp_path = resp_by_stem.get(req_path.stem)\n",
    "\n",
    "        route_ids = read_response_tokens(resp_path) if resp_path else []\n",
    "        coords_latlon, missing = map_response_to_coords(tasks, route_ids)\n",
    "\n",
    "        all_lat = np.array([v[0] for v in tasks.values()], float)\n",
    "        all_lon = np.array([v[1] for v in tasks.values()], float)\n",
    "        route_lat = np.array([c[0] for c in coords_latlon], float)\n",
    "        route_lon = np.array([c[1] for c in coords_latlon], float)\n",
    "\n",
    "        return {\n",
    "            \"req\": req_path,\n",
    "            \"resp\": resp_path,\n",
    "            \"tasks\": tasks,\n",
    "            \"all_lat\": all_lat,\n",
    "            \"all_lon\": all_lon,\n",
    "            \"route_latlon\": coords_latlon,\n",
    "            \"route_lat\": route_lat,\n",
    "            \"route_lon\": route_lon,\n",
    "            \"missing\": int(missing),\n",
    "            \"route_km\": float(route_length_km(coords_latlon)),\n",
    "        }\n",
    "\n",
    "    s_begin = load_session(first_req)\n",
    "    s_end = load_session(second_last_req)\n",
    "\n",
    "    # --- new/deleted nodes (compare by rounded coordinate key)\n",
    "    km_begin = key_map_from_tasks(s_begin[\"tasks\"], nd=6)\n",
    "    km_end = key_map_from_tasks(s_end[\"tasks\"], nd=6)\n",
    "\n",
    "    begin_keys = set(km_begin.keys())\n",
    "    end_keys = set(km_end.keys())\n",
    "\n",
    "    new_keys = end_keys - begin_keys\n",
    "    deleted_keys = begin_keys - end_keys\n",
    "\n",
    "    new_nodes_lat = np.array([km_end[k][0] for k in new_keys], float) if new_keys else np.array([], float)\n",
    "    new_nodes_lon = np.array([km_end[k][1] for k in new_keys], float) if new_keys else np.array([], float)\n",
    "\n",
    "    del_nodes_lat = np.array([km_begin[k][0] for k in deleted_keys], float) if deleted_keys else np.array([], float)\n",
    "    del_nodes_lon = np.array([km_begin[k][1] for k in deleted_keys], float) if deleted_keys else np.array([], float)\n",
    "\n",
    "    # --- new connections (segments in END route but not in BEGIN route)\n",
    "    seg_begin = route_segments_from_coords(s_begin[\"route_latlon\"], nd=6)\n",
    "    seg_end = route_segments_from_coords(s_end[\"route_latlon\"], nd=6)\n",
    "    new_segs = seg_end - seg_begin\n",
    "\n",
    "    # --- stable limits across both plots\n",
    "    all_lon = np.concatenate([s_begin[\"all_lon\"], s_end[\"all_lon\"]]) if len(s_begin[\"all_lon\"]) and len(s_end[\"all_lon\"]) else (s_begin[\"all_lon\"] if len(s_begin[\"all_lon\"]) else s_end[\"all_lon\"])\n",
    "    all_lat = np.concatenate([s_begin[\"all_lat\"], s_end[\"all_lat\"]]) if len(s_begin[\"all_lat\"]) and len(s_end[\"all_lat\"]) else (s_begin[\"all_lat\"] if len(s_begin[\"all_lat\"]) else s_end[\"all_lat\"])\n",
    "\n",
    "    x_min, x_max = float(np.min(all_lon)), float(np.max(all_lon))\n",
    "    y_min, y_max = float(np.min(all_lat)), float(np.max(all_lat))\n",
    "    pad_x = (x_max - x_min) * 0.05 if x_max > x_min else 0.01\n",
    "    pad_y = (y_max - y_min) * 0.05 if y_max > y_min else 0.01\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 7))\n",
    "\n",
    "    # -------------------- Plot 1: BEGIN --------------------\n",
    "    ax0 = plt.subplot(1, 2, 1)\n",
    "    ax0.set_title(f\"BEGIN (first request)\\n{pretty_title(s_begin['req'])}\")\n",
    "    ax0.set_xlabel(\"Longitude\")\n",
    "    ax0.set_ylabel(\"Latitude\")\n",
    "    ax0.set_xlim(x_min - pad_x, x_max + pad_x)\n",
    "    ax0.set_ylim(y_min - pad_y, y_max + pad_y)\n",
    "    ax0.grid(True)\n",
    "\n",
    "    ax0.scatter(s_begin[\"all_lon\"], s_begin[\"all_lat\"], s=BASE_DOT_SIZE, color=BASE_DOT_COLOR)\n",
    "    if len(s_begin[\"route_lon\"]) >= 2:\n",
    "        ax0.plot(s_begin[\"route_lon\"], s_begin[\"route_lat\"], linewidth=BASE_ROUTE_LW, color=BASE_ROUTE_COLOR)\n",
    "\n",
    "    ax0.text(\n",
    "        0.01, 0.99,\n",
    "        f\"tasks: {len(s_begin['all_lon'])}   route_nodes: {len(s_begin['route_lon'])}\\n\"\n",
    "        f\"missing_mapped: {s_begin['missing']}   route: {s_begin['route_km']:.3f}km\",\n",
    "        transform=ax0.transAxes, va=\"top\"\n",
    "    )\n",
    "\n",
    "    # -------------------- Plot 2: END (second_last) --------------------\n",
    "    ax1 = plt.subplot(1, 2, 2)\n",
    "    ax1.set_title(f\"END (second_last request)\\n{pretty_title(s_end['req'])}\")\n",
    "    ax1.set_xlabel(\"Longitude\")\n",
    "    ax1.set_ylabel(\"Latitude\")\n",
    "    ax1.set_xlim(x_min - pad_x, x_max + pad_x)\n",
    "    ax1.set_ylim(y_min - pad_y, y_max + pad_y)\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # base nodes\n",
    "    ax1.scatter(s_end[\"all_lon\"], s_end[\"all_lat\"], s=BASE_DOT_SIZE, color=BASE_DOT_COLOR)\n",
    "\n",
    "    # new nodes\n",
    "    if len(new_nodes_lon):\n",
    "        ax1.scatter(new_nodes_lon, new_nodes_lat, s=NEW_DOT_SIZE, color=NEW_DOT_COLOR)\n",
    "\n",
    "    # deleted nodes (X)\n",
    "    if len(del_nodes_lon):\n",
    "        ax1.scatter(del_nodes_lon, del_nodes_lat, s=DEL_X_SIZE, marker=\"x\", color=DEL_X_COLOR)\n",
    "\n",
    "    # base route line\n",
    "    if len(s_end[\"route_lon\"]) >= 2:\n",
    "        ax1.plot(\n",
    "            s_end[\"route_lon\"], s_end[\"route_lat\"],\n",
    "            linewidth=BASE_ROUTE_LW, alpha=BASE_ROUTE_ALPHA, color=BASE_ROUTE_COLOR\n",
    "        )\n",
    "\n",
    "    # overlay NEW connections (green, thicker)\n",
    "    for a, b in new_segs:\n",
    "        if a in km_end and b in km_end:\n",
    "            lat_a, lon_a = km_end[a]\n",
    "            lat_b, lon_b = km_end[b]\n",
    "        elif a in km_begin and b in km_begin:\n",
    "            lat_a, lon_a = km_begin[a]\n",
    "            lat_b, lon_b = km_begin[b]\n",
    "        else:\n",
    "            continue\n",
    "        ax1.plot([lon_a, lon_b], [lat_a, lat_b], color=NEW_EDGE_COLOR, linewidth=NEW_EDGE_LW)\n",
    "\n",
    "    # Legend (top right) using proxy artists so it stays clean\n",
    "    legend_handles = [\n",
    "        Line2D([], [], linestyle=\"none\", marker=\"o\", color=BASE_DOT_COLOR, label=\"Location\"),\n",
    "        Line2D([], [], linestyle=\"none\", marker=\"o\", color=NEW_DOT_COLOR,  label=\"New Location\"),\n",
    "        Line2D([], [], linestyle=\"none\", marker=\"x\", color=DEL_X_COLOR,    label=\"Deleted Location\"),\n",
    "        Line2D([], [], linestyle=\"-\",    color=BASE_ROUTE_COLOR, linewidth=BASE_ROUTE_LW, label=\"Route\"),\n",
    "        Line2D([], [], linestyle=\"-\",    color=NEW_EDGE_COLOR,   linewidth=NEW_EDGE_LW,  label=\"New Route\"),\n",
    "    ]\n",
    "    ax1.legend(handles=legend_handles, loc=\"upper right\", framealpha=0.9)\n",
    "\n",
    "    ax1.text(\n",
    "        0.01, 0.99,\n",
    "        f\"tasks: {len(s_end['all_lon'])}   route_nodes: {len(s_end['route_lon'])}\\n\"\n",
    "        f\"missing_mapped: {s_end['missing']}   route: {s_end['route_km']:.3f}km \\n\"\n",
    "        f\"new_nodes: {len(new_keys)}   deleted_nodes: {len(deleted_keys)}   new_edges: {len(new_segs)}\",\n",
    "        transform=ax1.transAxes, va=\"top\"\n",
    "    )\n",
    "\n",
    "    print(f\"Found request files: {len(req_files)}, response files: {len(resp_files)}\")\n",
    "    print(f\"begin request: {s_begin['req'].name}                               end request: {s_end['req'].name}\")\n",
    "    print(f\"begin response: {s_begin['resp'].name if s_begin['resp'] else 'MISSING'}                               end response: {s_end['resp'].name}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Duration:{time() - start1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c076d45-d477-46c6-aece-53a72b04784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CALL\n",
    "# ------------------------------------------------------------\n",
    "start1 = time()\n",
    "\n",
    "visualize_first_and_second_last(\n",
    "    data_dir=r\"C:\\Users\\dirk1\\0.SYNTRA1\\syntra_ds1\\Playground1\\opdracht_transport\\learning_driver_preferences\\data\",\n",
    "    route_prefix=\"0521_301\",\n",
    "    date_str=\"20220614\",\n",
    ")\n",
    "\n",
    "print(f\"Duration:{time() - start1:.3f}\")\n",
    "#1.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793cfdab-91d3-40b6-b3fd-7198f0fb8aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745dd8b-0287-4db5-a855-db18ccd88528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all plots\n",
    "# RUN ONCE â€” Minimal, readable \"Trim/Add + Route Order\" visualizer\n",
    "# - Uses request JSON + response TXT files with the naming pattern:\n",
    "#     ROUTE-YYYYMMDD-HHMMSS-NTASKS-NFIXED.json / .txt\n",
    "# - Shows: current nodes, added nodes, removed nodes, current route polyline + faint previous polyline\n",
    "#\n",
    "# External requirement:\n",
    "#   - safe_json_load(p: Path) must exist (you already have it in your big cell)\n",
    "\n",
    "from pathlib import Path\n",
    "import re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from time import time\n",
    "start1 = time()\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Filename parsing\n",
    "# -----------------------------\n",
    "FN_RE = re.compile(\n",
    "    r\"^(?P<route>[^-]+)-(?P<date>\\d{8})-(?P<time>\\d{6})-(?P<n_tasks>\\d+)-(?P<n_fixed>\\d+)\\.(?P<ext>json|txt)$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def safe_json_load(p: Path) -> dict:\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "def parse_meta(filename: str) -> dict | None:\n",
    "    m = FN_RE.match(filename)\n",
    "    if not m:\n",
    "        return None\n",
    "    return {\n",
    "        \"route\": m.group(\"route\"),\n",
    "        \"date\": m.group(\"date\"),\n",
    "        \"time\": m.group(\"time\"),\n",
    "        \"n_tasks_in_name\": int(m.group(\"n_tasks\")),\n",
    "        \"n_fixed_in_name\": int(m.group(\"n_fixed\")),\n",
    "        \"ext\": m.group(\"ext\").lower(),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# 2) File discovery (recursive)\n",
    "# -----------------------------\n",
    "def find_files(root: Path, ext: str, route: str, yyyymmdd: str) -> list[Path]:\n",
    "    hits = []\n",
    "    for p in root.rglob(f\"*.{ext}\"):\n",
    "        meta = parse_meta(p.name)\n",
    "        if meta and meta[\"route\"] == route and meta[\"date\"] == yyyymmdd:\n",
    "            hits.append(p)\n",
    "    hits.sort(key=lambda p: parse_meta(p.name)[\"time\"])\n",
    "    return hits\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Data readers\n",
    "# -----------------------------\n",
    "def node_key(lat, lon, nd=6) -> str | None:\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return None\n",
    "    return f\"{round(float(lat), nd)}|{round(float(lon), nd)}\"\n",
    "\n",
    "def read_response_tokens(txt_path: Path) -> list[int]:\n",
    "    txt = txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return [int(t) for t in re.findall(r\"\\d+\", txt)]\n",
    "\n",
    "def parse_request_tasks(json_path: Path) -> pd.DataFrame:\n",
    "    obj = safe_json_load(json_path)\n",
    "    tasks = obj.get(\"tasks\", []) or []\n",
    "    rows = []\n",
    "    for i, t in enumerate(tasks):\n",
    "        addr = t.get(\"address\", {}) or {}\n",
    "        tw = t.get(\"timeWindow\", {}) or {}\n",
    "        lat = addr.get(\"latitude\", np.nan)\n",
    "        lon = addr.get(\"longitude\", np.nan)\n",
    "        rows.append({\n",
    "            \"task_idx\": i,\n",
    "            \"task_id\": str(t.get(\"id\", \"\")).strip(),\n",
    "            \"latitude\": float(lat) if lat is not None else np.nan,\n",
    "            \"longitude\": float(lon) if lon is not None else np.nan,\n",
    "            \"tw_from\": tw.get(\"from\"),\n",
    "            \"tw_till\": tw.get(\"till\"),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"node_key\"] = [node_key(a, b) for a, b in zip(df[\"latitude\"], df[\"longitude\"])]\n",
    "    return df.dropna(subset=[\"node_key\"]).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Map response order -> coords (task_id vs task_idx)\n",
    "# -----------------------------\n",
    "def map_response_to_coords(resp_tokens: list[int], tasks_df: pd.DataFrame) -> tuple[np.ndarray, str, int]:\n",
    "    if not resp_tokens:\n",
    "        return np.empty((0, 2)), \"missing\", 0\n",
    "\n",
    "    # try tokens as task_id\n",
    "    toks_as_id = [str(t) for t in resp_tokens]\n",
    "    id_join = pd.DataFrame({\"task_id\": toks_as_id}).merge(\n",
    "        tasks_df[[\"task_id\", \"longitude\", \"latitude\"]],\n",
    "        on=\"task_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    coords_id = id_join[[\"longitude\", \"latitude\"]].to_numpy(float)\n",
    "    miss_id = int(np.isnan(coords_id).any(axis=1).sum())\n",
    "\n",
    "    # try tokens as task_idx\n",
    "    idx_join = pd.DataFrame({\"task_idx\": resp_tokens}).merge(\n",
    "        tasks_df[[\"task_idx\", \"longitude\", \"latitude\"]],\n",
    "        on=\"task_idx\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    coords_idx = idx_join[[\"longitude\", \"latitude\"]].to_numpy(float)\n",
    "    miss_idx = int(np.isnan(coords_idx).any(axis=1).sum())\n",
    "\n",
    "    if miss_id <= miss_idx:\n",
    "        return coords_id, \"task_id\", miss_id\n",
    "    return coords_idx, \"index\", miss_idx\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Route length (polyline length in lon/lat units)\n",
    "# -----------------------------\n",
    "def route_length(coords: np.ndarray) -> float:\n",
    "    if coords is None or len(coords) < 2:\n",
    "        return 0.0\n",
    "    d = np.diff(coords, axis=0)\n",
    "    seg = np.sqrt(d[:, 0] ** 2 + d[:, 1] ** 2)\n",
    "    return float(np.nansum(seg))\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Main visualizer\n",
    "# -----------------------------\n",
    "def run_visualization(\n",
    "    *,\n",
    "    data_dir: str,\n",
    "    route_prefix: str,\n",
    "    yyyymmdd: str,\n",
    "    requests_subdir: str = \"requests\",\n",
    "    responses_subdir: str = \"responses\",\n",
    "    interval_ms: int = 1200,\n",
    "    show_table: bool = False,\n",
    "    #save_gif: bool = False,\n",
    "    #gif_name: str | None = None,\n",
    "):\n",
    "    data_dir = Path(data_dir)\n",
    "    req_root = data_dir / requests_subdir\n",
    "    resp_root = data_dir / responses_subdir\n",
    "\n",
    "    req_files = find_files(req_root, \"json\", route_prefix, yyyymmdd)\n",
    "    resp_files = find_files(resp_root, \"txt\", route_prefix, yyyymmdd)\n",
    "    if not req_files:\n",
    "        raise FileNotFoundError(f\"No request JSON files for {route_prefix} on {yyyymmdd} under {req_root}\")\n",
    "    if not resp_files:\n",
    "        raise FileNotFoundError(f\"No response TXT files for {route_prefix} on {yyyymmdd} under {resp_root}\")\n",
    "\n",
    "    resp_by_stem = {p.stem: p for p in resp_files}\n",
    "\n",
    "    # ---- build sessions list\n",
    "    sessions = []\n",
    "    for req in req_files:\n",
    "        meta = parse_meta(req.name)\n",
    "        tasks_df = parse_request_tasks(req)\n",
    "\n",
    "        resp = resp_by_stem.get(req.stem)\n",
    "        tokens = read_response_tokens(resp) if resp else []\n",
    "        coords, resp_type, miss = map_response_to_coords(tokens, tasks_df)\n",
    "\n",
    "        sessions.append({\n",
    "            \"meta\": {\n",
    "                \"date\": meta[\"date\"],\n",
    "                \"time\": meta[\"time\"],\n",
    "                \"file\": req.name,\n",
    "                \"stem\": req.stem,\n",
    "                \"n_fixed\": meta[\"n_fixed_in_name\"],\n",
    "                \"n_tasks\": int(len(tasks_df)),\n",
    "                \"resp_type\": resp_type,\n",
    "                \"miss\": int(miss),\n",
    "                \"route_len\": route_length(coords),\n",
    "            },\n",
    "            \"tasks_df\": tasks_df,\n",
    "            \"coords\": coords,\n",
    "        })\n",
    "\n",
    "    if show_table:\n",
    "        display(pd.DataFrame([s[\"meta\"] for s in sessions]))\n",
    "\n",
    "    # ---- precompute per-frame arrays (added/removed + current + route lines)\n",
    "    added_xy, removed_xy, current_xy = [], [], []\n",
    "    route_xy, prev_route_xy = [], []\n",
    "    labels, cur_len, prev_len = [], [], []\n",
    "\n",
    "    for i, s in enumerate(sessions):\n",
    "        df = s[\"tasks_df\"]\n",
    "        cur_keys = set(df[\"node_key\"])\n",
    "\n",
    "        if i == 0:\n",
    "            prev_df = df.iloc[0:0]\n",
    "            prev_keys = set()\n",
    "            prev_coords = np.empty((0, 2))\n",
    "        else:\n",
    "            prev_df = sessions[i - 1][\"tasks_df\"]\n",
    "            prev_keys = set(prev_df[\"node_key\"])\n",
    "            prev_coords = sessions[i - 1][\"coords\"]\n",
    "\n",
    "        add_keys = cur_keys - prev_keys\n",
    "        rem_keys = prev_keys - cur_keys\n",
    "\n",
    "        added_xy.append(df[df[\"node_key\"].isin(add_keys)][[\"longitude\", \"latitude\"]].to_numpy(float))\n",
    "        removed_xy.append(prev_df[prev_df[\"node_key\"].isin(rem_keys)][[\"longitude\", \"latitude\"]].to_numpy(float))\n",
    "        current_xy.append(df[[\"longitude\", \"latitude\"]].to_numpy(float))\n",
    "\n",
    "        route_xy.append(s[\"coords\"])\n",
    "        prev_route_xy.append(prev_coords)\n",
    "\n",
    "        cur_len.append(route_length(s[\"coords\"]))\n",
    "        prev_len.append(route_length(prev_coords))\n",
    "\n",
    "        t = s[\"meta\"][\"time\"]\n",
    "        labels.append(\n",
    "            f\"{s['meta']['date']} {t[:2]}:{t[2:4]}:{t[4:6]}  \"\n",
    "            f\"n={s['meta']['n_tasks']} fixed={s['meta']['n_fixed']} resp={s['meta']['resp_type']} miss={s['meta']['miss']}\"\n",
    "        )\n",
    "\n",
    "    # stable axis limits\n",
    "    all_pts = np.vstack([xy for xy in current_xy if len(xy)])\n",
    "    x_min, y_min = np.nanmin(all_pts, axis=0)\n",
    "    x_max, y_max = np.nanmax(all_pts, axis=0)\n",
    "    pad_x = (x_max - x_min) * 0.05 if x_max > x_min else 0.01\n",
    "    pad_y = (y_max - y_min) * 0.05 if y_max > y_min else 0.01\n",
    "\n",
    "    # ---- plot elements\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Trim/Add + Route Order (current + previous)\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    ax.set_xlim(x_min - pad_x, x_max + pad_x)\n",
    "    ax.set_ylim(y_min - pad_y, y_max + pad_y)\n",
    "    ax.grid(True)\n",
    "\n",
    "    sc_cur = ax.scatter([], [], s=10, marker=\"o\")\n",
    "    sc_add = ax.scatter([], [], s=30, marker=\"o\")\n",
    "    sc_rem = ax.scatter([], [], s=60, marker=\"x\")\n",
    "\n",
    "    ln_prev, = ax.plot([], [], linewidth=1, alpha=0.10)\n",
    "    ln_route, = ax.plot([], [], linewidth=1, alpha=1.00)\n",
    "\n",
    "    txt = ax.text(0.01, 0.99, \"\", transform=ax.transAxes, va=\"top\")\n",
    "\n",
    "    def init():\n",
    "        sc_cur.set_offsets(np.empty((0, 2)))\n",
    "        sc_add.set_offsets(np.empty((0, 2)))\n",
    "        sc_rem.set_offsets(np.empty((0, 2)))\n",
    "        ln_prev.set_data([], [])\n",
    "        ln_route.set_data([], [])\n",
    "        txt.set_text(\"\")\n",
    "        return sc_cur, sc_add, sc_rem, ln_prev, ln_route, txt\n",
    "\n",
    "    def update(i):\n",
    "        sc_cur.set_offsets(current_xy[i] if len(current_xy[i]) else np.empty((0, 2)))\n",
    "        sc_add.set_offsets(added_xy[i] if len(added_xy[i]) else np.empty((0, 2)))\n",
    "        sc_rem.set_offsets(removed_xy[i] if len(removed_xy[i]) else np.empty((0, 2)))\n",
    "\n",
    "        if len(prev_route_xy[i]):\n",
    "            ln_prev.set_data(prev_route_xy[i][:, 0], prev_route_xy[i][:, 1])\n",
    "        else:\n",
    "            ln_prev.set_data([], [])\n",
    "\n",
    "        if len(route_xy[i]):\n",
    "            ln_route.set_data(route_xy[i][:, 0], route_xy[i][:, 1])\n",
    "        else:\n",
    "            ln_route.set_data([], [])\n",
    "\n",
    "        txt.set_text(\n",
    "            f\"{labels[i]}\\n\"\n",
    "            f\"added: {len(added_xy[i])} removed: {len(removed_xy[i])}\\n\"\n",
    "            f\"len(prev): {prev_len[i]:.6f}    len(cur): {cur_len[i]:.6f}\"\n",
    "        )\n",
    "        return sc_cur, sc_add, sc_rem, ln_prev, ln_route, txt\n",
    "\n",
    "    anim = FuncAnimation(fig, update, frames=len(sessions), init_func=init, interval=interval_ms, blit=True)\n",
    "    display(HTML(anim.to_jshtml()))\n",
    "\n",
    "#    if save_gif:\n",
    "#        if gif_name is None:\n",
    "#            gif_name = f\"trim_add_{route_prefix}_{yyyymmdd}.gif\"\n",
    "#        out_gif = data_dir / gif_name\n",
    "#        anim.save(out_gif, writer=PillowWriter(fps=max(1, int(1000 / interval_ms))))\n",
    "#        print(\"Saved GIF:\", out_gif)\n",
    "\n",
    "    return anim\n",
    "\n",
    "\n",
    "print(f\"Duration:{time() - start1:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e297ed-a431-43d0-add3-bcf9876c2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start1 = time()\n",
    "\n",
    "_ = run_visualization(\n",
    "    data_dir=r\"C:\\Users\\dirk1\\0.SYNTRA1\\syntra_ds1\\Playground1\\opdracht_transport\\learning_driver_preferences\\data\",\n",
    "    route_prefix=\"0521_301\",\n",
    "    yyyymmdd=\"20220614\",\n",
    "    #save_gif=False,\n",
    ")\n",
    "\n",
    "print(f\"Duration:{time() - start1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5acf6-5b01-4b00-a774-2841b582375f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10(syntra_ds1)",
   "language": "python",
   "name": "syntra_ds1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
